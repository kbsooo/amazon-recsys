{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon RecSys GNN - Training Notebook\n",
    "This notebook is auto-generated for Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Random Seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions (from src/model.py)\n",
    "#%%\n\"\"\"\n모델 정의 - Amazon RecSys GNN\nLightGCN 베이스 모델 + Rating Prediction 모델 + 앙상블\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#%%\nprint(\"=\"*60)\nprint(\"LightGCN 모델 정의\")\nprint(\"=\"*60)\n\nclass LightGCN(nn.Module):\n    \"\"\"\n    LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n    \n    구조 학습(Structure Learning)에 집중하는 모델.\n    BPR Loss로 학습하여 이진 추천 태스크에 활용.\n    \"\"\"\n    def __init__(self, n_users, n_items, emb_dim=64, n_layers=3):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.emb_dim = emb_dim\n        self.n_layers = n_layers\n        \n        # Embedding Layers (Xavier initialization)\n        self.user_emb = nn.Embedding(n_users, emb_dim)\n        self.item_emb = nn.Embedding(n_items, emb_dim)\n        nn.init.xavier_uniform_(self.user_emb.weight)\n        nn.init.xavier_uniform_(self.item_emb.weight)\n    \n    def forward(self, edge_index, edge_weight):\n        \"\"\"\n        Graph Convolution 수행\n        \n        Args:\n            edge_index: [2, num_edges] - Edge Index\n            edge_weight: [num_edges] - Normalized Edge Weights\n        \n        Returns:\n            user_emb: [n_users, emb_dim]\n            item_emb: [n_items, emb_dim]\n        \"\"\"\n        # 초기 임베딩\n        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n        embs = [all_emb]\n        \n        # Layer-wise Propagation\n        for _ in range(self.n_layers):\n            row, col = edge_index\n            # Weighted Message Passing\n            messages = all_emb[col] * edge_weight.unsqueeze(1)\n            \n            # Aggregation (scatter_add)\n            all_emb = torch.zeros_like(all_emb).scatter_add(\n                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n            )\n            embs.append(all_emb)\n        \n        # Layer Combination (Mean pooling)\n        final_emb = torch.mean(torch.stack(embs), dim=0)\n        \n        # User/Item 분리\n        user_emb = final_emb[:self.n_users]\n        item_emb = final_emb[self.n_users:]\n        \n        return user_emb, item_emb\n    \n    def predict(self, user_idx, item_idx, edge_index, edge_weight):\n        \"\"\"\n        유저-아이템 쌍에 대한 예측 점수\n        \n        Args:\n            user_idx: [batch_size] or single user\n            item_idx: [batch_size] or single item\n        \n        Returns:\n            scores: [batch_size]\n        \"\"\"\n        user_emb, item_emb = self(edge_index, edge_weight)\n        scores = (user_emb[user_idx] * item_emb[item_idx]).sum(dim=-1)\n        return scores\n\nprint(\"✅ LightGCN 클래스 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"LightGCN + Rating Prediction 모델 정의\")\nprint(\"=\"*60)\n\nclass LightGCN_Rating(nn.Module):\n    \"\"\"\n    LightGCN + Rating Prediction Head\n    \n    구조 학습 + 평점 예측을 동시에 수행.\n    Multi-task Learning으로 일반화 성능 향상.\n    \"\"\"\n    def __init__(self, n_users, n_items, emb_dim=64, n_layers=3):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.emb_dim = emb_dim\n        self.n_layers = n_layers\n        \n        # Embedding Layers\n        self.user_emb = nn.Embedding(n_users, emb_dim)\n        self.item_emb = nn.Embedding(n_items, emb_dim)\n        nn.init.xavier_uniform_(self.user_emb.weight)\n        nn.init.xavier_uniform_(self.item_emb.weight)\n        \n        # Rating Prediction MLP\n        self.rating_mlp = nn.Sequential(\n            nn.Linear(emb_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 1)\n        )\n    \n    def forward(self, edge_index, edge_weight):\n        \"\"\"Graph Convolution\"\"\"\n        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n        embs = [all_emb]\n        \n        for _ in range(self.n_layers):\n            row, col = edge_index\n            messages = all_emb[col] * edge_weight.unsqueeze(1)\n            all_emb = torch.zeros_like(all_emb).scatter_add(\n                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n            )\n            embs.append(all_emb)\n        \n        final_emb = torch.mean(torch.stack(embs), dim=0)\n        user_emb = final_emb[:self.n_users]\n        item_emb = final_emb[self.n_users:]\n        \n        return user_emb, item_emb\n    \n    def predict_rating(self, user_idx, item_idx, edge_index, edge_weight):\n        \"\"\"\n        평점 예측 (1.0 ~ 5.0 범위)\n        \n        Returns:\n            ratings: [batch_size] - 예측 평점\n        \"\"\"\n        user_emb, item_emb = self(edge_index, edge_weight)\n        interaction = user_emb[user_idx] * item_emb[item_idx]\n        rating_logit = self.rating_mlp(interaction).squeeze(-1)\n        \n        # Sigmoid로 [0, 1] 변환 후 [0.5, 5.0] 범위로 스케일링\n        rating = torch.sigmoid(rating_logit) * 4.5 + 0.5\n        return rating\n    \n    def predict(self, user_idx, item_idx, edge_index, edge_weight):\n        \"\"\"구조 기반 점수 (BPR용)\"\"\"\n        user_emb, item_emb = self(edge_index, edge_weight)\n        scores = (user_emb[user_idx] * item_emb[item_idx]).sum(dim=-1)\n        return scores\n\nprint(\"✅ LightGCN_Rating 클래스 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"모델 테스트\")\nprint(\"=\"*60)\n\n# 더미 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline (from scripts/03_train.py)\n",
    "#%%\n\"\"\"\n학습 파이프라인 - Amazon RecSys GNN\nLightGCN + Rating Prediction 모델 학습\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nimport warnings\nfrom pathlib import Path\nimport sys\n\n# 모듈 경로 추가\n\nwarnings.filterwarnings('ignore')\n\n# 한글 폰트\nplt.rc('font', family='AppleGothic')\nplt.rcParams['axes.unicode_minus'] = False\n\n#%%\nprint(\"=\"*60)\nprint(\"1. 환경 설정\")\nprint(\"=\"*60)\n\n# Random Seed\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nprint(f\"Device: {device}\")\n\n# Hyperparameters\nEMB_DIM = 64\nN_LAYERS = 3\nLR = 1e-3\nWEIGHT_DECAY = 1e-5\nEPOCHS = 100\nBATCH_SIZE = 2048\nNUM_NEG = 4  # Negative samples per positive\nHARD_NEG_RATIO = 0.5\nLAMBDA_MSE = 0.5  # Rating loss weight (for CCB model)\n\nprint(f\"\\n하이퍼파라미터:\")\nprint(f\"  임베딩 차원: {EMB_DIM}\")\nprint(f\"  레이어 수: {N_LAYERS}\")\nprint(f\"  학습률: {LR}\")\nprint(f\"  배치 크기: {BATCH_SIZE}\")\nprint(f\"  Epochs: {EPOCHS}\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. 데이터 로드\")\nprint(\"=\"*60)\n\n# 분할 데이터\ntrain_df = pd.read_csv('/kaggle/input/amazon/train_split.csv')\nval_df = pd.read_csv('/kaggle/input/amazon/val_split.csv')\ntest_df = pd.read_csv('/kaggle/input/amazon/test_split.csv')\n\nprint(f\"Train: {len(train_df):,}\")\nprint(f\"Val: {len(val_df):,}\")\nprint(f\"Test: {len(test_df):,}\")\n\n# 그래프 데이터\ngraph_data = torch.load('/kaggle/input/amazon/train_graph.pt')\nedge_index = graph_data['edge_index'].to(device)\ncca_edge_weight = graph_data['cca_weight'].to(device)\nccb_edge_weight = graph_data['ccb_weight'].to(device)\nn_users = graph_data['n_users']\nn_items = graph_data['n_items']\n\nprint(f\"\\n그래프 정보:\")\nprint(f\"  유저 수: {n_users:,}\")\nprint(f\"  아이템 수: {n_items:,}\")\nprint(f\"  엣지 수: {edge_index.shape[1]:,}\")\n\n# Edge Sets\nwith open('/kaggle/input/amazon/all_known_edges.pkl', 'rb') as f:\n    all_known_edges = pickle.load(f)\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. Negative Sampling 함수\")\nprint(\"=\"*60)\n\ndef fast_sample_negatives(batch_size, num_neg=4):\n    \"\"\"빠른 랜덤 negative sampling\"\"\"\n    return torch.randint(0, n_items, (batch_size, num_neg), device=device)\n\n@torch.no_grad()\ndef hard_negative_sampling(user_emb, item_emb, pos_users, num_neg=4, num_candidates=50):\n    \"\"\"\n    Hard Negative Sampling\n    높은 점수를 가진 negative를 선택하여 학습 효율 향상\n    \"\"\"\n    batch_size = len(pos_users)\n    candidates = torch.randint(0, n_items, (batch_size, num_candidates), device=device)\n    \n    user_expanded = user_emb[pos_users].unsqueeze(1)  # [B, 1, D]\n    item_candidates = item_emb[candidates]  # [B, C, D]\n    scores = (user_expanded * item_candidates).sum(dim=2)  # [B, C]\n    \n    # Top-K as hard negatives\n    _, top_indices = scores.topk(num_neg, dim=1)\n    hard_negs = candidates.gather(1, top_indices)\n    \n    return hard_negs\n\nprint(\"✅ Negative Sampling 함수 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. 평가 함수\")\nprint(\"=\"*60)\n\n@torch.no_grad()\ndef evaluate_recall_ndcg(model, eval_df, edge_index, edge_weight, k_list=[20, 50]):\n    \"\"\"\n    Recall@K 및 NDCG@K 평가\n    \"\"\"\n    model.eval()\n    u_emb, i_emb = model(edge_index, edge_weight)\n    \n    # User별로 그룹화\n    user_groups = eval_df.groupby('user_idx')\n    \n    recall_at_k = {k: [] for k in k_list}\n    ndcg_at_k = {k: [] for k in k_list}\n    \n    for user_idx, group in user_groups:\n        # Ground truth items\n        gt_items = set(group['item_idx'].values)\n        \n        # 모든 아이템에 대한 점수 계산\n        user_vec = u_emb[user_idx]\n        scores = (user_vec @ i_emb.t()).cpu().numpy()\n        \n        # Top-K 추천\n        for k in k_list:\n            top_k_items = np.argsort(scores)[-k:][::-1]\n            \n            # Recall@K\n            hits = len(set(top_k_items) & gt_items)\n            recall = hits / len(gt_items) if len(gt_items) > 0 else 0\n            recall_at_k[k].append(recall)\n            \n            # NDCG@K\n            dcg = sum([1 / np.log2(i + 2) if item in gt_items else 0 \n                      for i, item in enumerate(top_k_items)])\n            idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(gt_items)))])\n            ndcg = dcg / idcg if idcg > 0 else 0\n            ndcg_at_k[k].append(ndcg)\n    \n    # 평균 계산\n    metrics = {}\n    for k in k_list:\n        metrics[f'Recall@{k}'] = np.mean(recall_at_k[k])\n        metrics[f'NDCG@{k}'] = np.mean(ndcg_at_k[k])\n    \n    return metrics\n\nprint(\"✅ 평가 함수 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"5. CCA 모델 학습 (Binary Recommendation)\")\nprint(\"=\"*60)\n\n# 모델 초기화\ncca_model = LightGCN(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\noptimizer_cca = AdamW(cca_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler_cca = CosineAnnealingLR(optimizer_cca, T_max=EPOCHS)\n\n# 학습 데이터\ntrain_users = torch.LongTensor(train_df['user_idx'].values).to(device)\ntrain_items = torch.LongTensor(train_df['item_idx'].values).to(device)\ntrain_ratings = torch.FloatTensor(train_df['rating'].values).to(device)\n\n# Rating weight (높은 평점에 더 큰 가중치)\ntrain_weights = 0.5 + 0.15 * (train_ratings - train_ratings.mean())\ntrain_weights = train_weights.to(device)\n\n# 학습 이력\nhistory_cca = {'loss': [], 'val_recall@20': [], 'val_ndcg@20': []}\n\nprint(\"학습 시작...\")\nbest_val_recall = 0\npatience_counter = 0\nPATIENCE = 10\n\nfor epoch in range(EPOCHS):\n    cca_model.train()\n    perm = torch.randperm(len(train_users), device=device)\n    epoch_loss = 0\n    n_batches = 0\n    \n    for i in range(0, len(train_users), BATCH_SIZE):\n        batch_idx = perm[i:i+BATCH_SIZE]\n        pos_u = train_users[batch_idx]\n        pos_i = train_items[batch_idx]\n        weights = train_weights[batch_idx]\n        \n        # Forward\n        u_emb, i_emb = cca_model(edge_index, cca_edge_weight)\n        \n        # Hard + Random Negative Sampling\n        n_hard = int(NUM_NEG * HARD_NEG_RATIO)\n        hard_negs = hard_negative_sampling(u_emb, i_emb, pos_u, num_neg=n_hard)\n        rand_negs = fast_sample_negatives(len(batch_idx), NUM_NEG - n_hard)\n        neg_i = torch.cat([hard_negs, rand_negs], dim=1)\n        \n        # Weighted BPR Loss\n        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n        neg_scores = (u_emb[pos_u].unsqueeze(1) * i_emb[neg_i]).sum(dim=2)\n        \n        diff = pos_scores.unsqueeze(1) - neg_scores\n        loss_per_sample = -torch.log(torch.sigmoid(diff) + 1e-8).mean(dim=1)\n        loss = (loss_per_sample * weights).mean()\n        \n        # Backward\n        optimizer_cca.zero_grad()\n        loss.backward()\n        optimizer_cca.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n    \n    scheduler_cca.step()\n    avg_loss = epoch_loss / n_batches\n    history_cca['loss'].append(avg_loss)\n    \n    # Validation (매 5 epoch)\n    if (epoch + 1) % 5 == 0:\n        val_metrics = evaluate_recall_ndcg(cca_model, val_df, edge_index, cca_edge_weight, k_list=[20])\n        history_cca['val_recall@20'].append(val_metrics['Recall@20'])\n        history_cca['val_ndcg@20'].append(val_metrics['NDCG@20'])\n        \n        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | \"\n              f\"Val Recall@20: {val_metrics['Recall@20']:.4f} | \"\n              f\"Val NDCG@20: {val_metrics['NDCG@20']:.4f}\")\n        \n        # Early Stopping\n        if val_metrics['Recall@20'] > best_val_recall:\n            best_val_recall = val_metrics['Recall@20']\n            patience_counter = 0\n            # 모델 저장\n            torch.save(cca_model.state_dict(), 'models/cca_best.pt')\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"\\n✅ CCA 모델 학습 완료 (Best Val Recall@20: {best_val_recall:.4f})\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"6. CCB 모델 학습 (Rating Prediction + BPR)\")\nprint(\"=\"*60)\n\n# 모델 초기화\nccb_model = LightGCN_Rating(n_users, n_items, EMB_DIM, N_LAYERS).to(device)\noptimizer_ccb = AdamW(ccb_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler_ccb = CosineAnnealingLR(optimizer_ccb, T_max=EPOCHS)\n\n# 학습 이력\nhistory_ccb = {'loss': [], 'bpr_loss': [], 'mse_loss': [], 'val_rmse': []}\n\nprint(\"학습 시작...\")\nbest_val_rmse = float('inf')\npatience_counter = 0\n\nfor epoch in range(EPOCHS):\n    ccb_model.train()\n    perm = torch.randperm(len(train_users), device=device)\n    epoch_loss = 0\n    epoch_bpr = 0\n    epoch_mse = 0\n    n_batches = 0\n    \n    for i in range(0, len(train_users), BATCH_SIZE):\n        batch_idx = perm[i:i+BATCH_SIZE]\n        pos_u = train_users[batch_idx]\n        pos_i = train_items[batch_idx]\n        pos_r = train_ratings[batch_idx]\n        \n        # Forward\n        u_emb, i_emb = ccb_model(edge_index, ccb_edge_weight)\n        \n        # Negative Sampling (BPR)\n        neg_i = fast_sample_negatives(len(batch_idx), NUM_NEG)\n        \n        # BPR Loss\n        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n        neg_scores = (u_emb[pos_u].unsqueeze(1) * i_emb[neg_i]).sum(dim=2)\n        loss_bpr = -torch.log(torch.sigmoid(pos_scores.unsqueeze(1) - neg_scores) + 1e-8).mean()\n        \n        # MSE Loss (Rating Prediction)\n        pred_r = ccb_model.predict_rating(pos_u, pos_i, edge_index, ccb_edge_weight)\n        loss_mse = F.mse_loss(pred_r, pos_r)\n        \n        # Total Loss\n        loss = loss_bpr + LAMBDA_MSE * loss_mse\n        \n        # Backward\n        optimizer_ccb.zero_grad()\n        loss.backward()\n        optimizer_ccb.step()\n        \n        epoch_loss += loss.item()\n        epoch_bpr += loss_bpr.item()\n        epoch_mse += loss_mse.item()\n        n_batches += 1\n    \n    scheduler_ccb.step()\n    avg_loss = epoch_loss / n_batches\n    avg_bpr = epoch_bpr / n_batches\n    avg_mse = epoch_mse / n_batches\n    \n    history_ccb['loss'].append(avg_loss)\n    history_ccb['bpr_loss'].append(avg_bpr)\n    history_ccb['mse_loss'].append(avg_mse)\n    \n    # Validation (매 5 epoch)\n    if (epoch + 1) % 5 == 0:\n        ccb_model.eval()\n        with torch.no_grad():\n            val_u = torch.LongTensor(val_df['user_idx'].values).to(device)\n            val_i = torch.LongTensor(val_df['item_idx'].values).to(device)\n            val_r = torch.FloatTensor(val_df['rating'].values).to(device)\n            \n            val_pred = ccb_model.predict_rating(val_u, val_i, edge_index, ccb_edge_weight)\n            val_rmse = torch.sqrt(F.mse_loss(val_pred, val_r)).item()\n            history_ccb['val_rmse'].append(val_rmse)\n            \n        print(f\"Epoch {epoch+1}/{EPOCHS} | Total Loss: {avg_loss:.4f} | \"\n              f\"BPR: {avg_bpr:.4f} | MSE: {avg_mse:.4f} | Val RMSE: {val_rmse:.4f}\")\n        \n        # Early Stopping\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            patience_counter = 0\n            torch.save(ccb_model.state_dict(), 'models/ccb_best.pt')\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"\\n✅ CCB 모델 학습 완료 (Best Val RMSE: {best_val_rmse:.4f})\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"7. 학습 곡선 시각화\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# CCA Loss\naxes[0, 0].plot(history_cca['loss'], label='Train Loss', color='blue')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('CCA Training Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# CCA Metrics\nif history_cca['val_recall@20']:\n    x = np.arange(5, len(history_cca['loss']) + 1, 5)[:len(history_cca['val_recall@20'])]\n    axes[0, 1].plot(x, history_cca['val_recall@20'], label='Recall@20', marker='o')\n    axes[0, 1].plot(x, history_cca['val_ndcg@20'], label='NDCG@20', marker='s')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Score')\n    axes[0, 1].set_title('CCA Validation Metrics')\n    axes[0, 1].legend()\n    axes[0, 1].grid(alpha=0.3)\n\n# CCB Losses\naxes[1, 0].plot(history_ccb['bpr_loss'], label='BPR Loss', color='orange')\naxes[1, 0].plot(history_ccb['mse_loss'], label='MSE Loss', color='green')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].set_title('CCB Training Losses')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# CCB RMSE\nif history_ccb['val_rmse']:\n    x = np.arange(5, len(history_ccb['loss']) + 1, 5)[:len(history_ccb['val_rmse'])]\n    axes[1, 1].plot(x, history_ccb['val_rmse'], label='Val RMSE', marker='o', color='red')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('RMSE')\n    axes[1, 1].set_title('CCB Validation RMSE')\n    axes[1, 1].legend()\n    axes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('outputs/training_curves.png', dpi=150)\nplt.show()\n\nprint(\"✅ 학습 곡선 저장 완료 (outputs/training_curves.png)\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"8. 최종 모델 저장\")\nprint(\"=\"*60)\n\n# 최종 모델 로드 (Best validation)\ncca_model.load_state_dict(torch.load('models/cca_best.pt'))\nccb_model.load_state_dict(torch.load('models/ccb_best.pt'))\n\n# 전체 저장\ntorch.save({\n    'cca_state_dict': cca_model.state_dict(),\n    'ccb_state_dict': ccb_model.state_dict(),\n    'config': {\n        'n_users': n_users,\n        'n_items': n_items,\n        'emb_dim': EMB_DIM,\n        'n_layers': N_LAYERS\n    },\n    'history': {\n        'cca': history_cca,\n        'ccb': history_ccb\n    }\n}, 'models/ensemble_model.pt')\n\nprint(\"✅ 모델 저장 완료:\")\nprint(\"  - models/cca_best.pt\")\nprint(\"  - models/ccb_best.pt\")\nprint(\"  - models/ensemble_model.pt\")\n\nprint(\"\\n✅ 학습 파이프라인 실행 완료!\")\n\n# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}