{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon RecSys GNN - SimGCL Training\n",
    "## Simple Graph Contrastive Learning with Weighted BPR Loss\n",
    "\n",
    "This notebook implements:\n",
    "- **SimGCL**: Contrastive Learning to overcome data sparsity\n",
    "- **Weighted BPR Loss**: Rating information as loss weights\n",
    "- **Combined Strategy**: Sparsity solution + Rating utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Random Seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions (SimGCL)\n",
    "print(\"LightGCN + SimGCL (Contrastive Learning) 모델 정의\")\nprint(\"=\"*60)\n\nclass LightGCN_SimGCL(nn.Module):\n    \"\"\"\n    LightGCN with Simple Graph Contrastive Learning (SimGCL)\n    \n    희소한 데이터에서 강건한 임베딩을 학습하기 위해 Contrastive Learning 적용.\n    학습 시 임베딩에 노이즈를 추가하여 데이터 증강 효과를 얻습니다.\n    \"\"\"\n    def __init__(self, n_users, n_items, emb_dim=64, n_layers=3, eps=0.1):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.emb_dim = emb_dim\n        self.n_layers = n_layers\n        self.eps = eps  # Noise level for perturbation\n        \n        # Embedding Layers\n        self.user_emb = nn.Embedding(n_users, emb_dim)\n        self.item_emb = nn.Embedding(n_items, emb_dim)\n        nn.init.xavier_uniform_(self.user_emb.weight)\n        nn.init.xavier_uniform_(self.item_emb.weight)\n    \n    def forward(self, edge_index, edge_weight, perturbed=False):\n        \"\"\"\n        Graph Convolution 수행\n        \n        Args:\n            edge_index: [2, num_edges]\n            edge_weight: [num_edges]\n            perturbed: True이면 노이즈가 섞인 임베딩 반환\n        \n        Returns:\n            user_emb: [n_users, emb_dim]\n            item_emb: [n_items, emb_dim]\n        \"\"\"\n        # 초기 임베딩\n        all_emb = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n        \n        # Perturbation (노이즈 추가)\n        if perturbed and self.training:\n            random_noise = torch.randn_like(all_emb).to(all_emb.device)\n            all_emb = all_emb + torch.sign(all_emb) * F.normalize(random_noise, dim=-1) * self.eps\n        \n        embs = [all_emb]\n        \n        # Layer-wise Propagation\n        for _ in range(self.n_layers):\n            row, col = edge_index\n            messages = all_emb[col] * edge_weight.unsqueeze(1)\n            all_emb = torch.zeros_like(all_emb).scatter_add(\n                0, row.unsqueeze(1).expand(-1, self.emb_dim), messages\n            )\n            embs.append(all_emb)\n        \n        # Layer Combination\n        final_emb = torch.mean(torch.stack(embs), dim=0)\n        user_emb = final_emb[:self.n_users]\n        item_emb = final_emb[self.n_users:]\n        \n        return user_emb, item_emb\n    \n    def get_perturbed_embeddings(self, edge_index, edge_weight):\n        \"\"\"\n        두 개의 perturbed view 생성 (Contrastive Learning용)\n        \"\"\"\n        u_emb_1, i_emb_1 = self.forward(edge_index, edge_weight, perturbed=True)\n        u_emb_2, i_emb_2 = self.forward(edge_index, edge_weight, perturbed=True)\n        return u_emb_1, i_emb_1, u_emb_2, i_emb_2\n    \n    def predict(self, user_idx, item_idx, edge_index, edge_weight):\n        \"\"\"예측 점수 (추론 시 사용, perturbed=False)\"\"\"\n        user_emb, item_emb = self.forward(edge_index, edge_weight, perturbed=False)\n        scores = (user_emb[user_idx] * item_emb[item_idx]).sum(dim=-1)\n        return scores\n\nprint(\"✅ LightGCN_SimGCL 클래스 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"InfoNCE Loss (Contrastive Loss) 함수 정의\")\nprint(\"=\"*60)\n\ndef compute_infonce_loss(emb_1, emb_2, temperature=0.2):\n    \"\"\"\n    InfoNCE Loss for Contrastive Learning\n    \n    Args:\n        emb_1: [batch_size, emb_dim] - First view\n        emb_2: [batch_size, emb_dim] - Second view\n        temperature: Temperature parameter\n    \n    Returns:\n        loss: Scalar contrastive loss\n    \"\"\"\n    # Normalize embeddings\n    emb_1 = F.normalize(emb_1, dim=-1)\n    emb_2 = F.normalize(emb_2, dim=-1)\n    \n    batch_size = emb_1.shape[0]\n    \n    # Positive pairs: (emb_1[i], emb_2[i])\n    pos_score = (emb_1 * emb_2).sum(dim=-1) / temperature  # [batch_size]\n    \n    # All pairs: emb_1 @ emb_2.T\n    all_scores = torch.matmul(emb_1, emb_2.t()) / temperature  # [batch_size, batch_size]\n    \n    # InfoNCE: -log(exp(pos) / sum(exp(all)))\n    loss = -torch.log(\n        torch.exp(pos_score) / torch.exp(all_scores).sum(dim=1)\n    ).mean()\n    \n    return loss\n\nprint(\"✅ InfoNCE Loss 함수 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"모델 테스트\")\nprint(\"=\"*60)\n\n# 더미 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Pipeline (SimGCL)\n",
    "#%%\n\"\"\"\nSimGCL 학습 파이프라인 - Amazon RecSys GNN\nLightGCN + Sim GCL (Contrastive Learning) + Weighted BPR Loss\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nimport warnings\nfrom pathlib import Path\nimport sys\n\n# 모듈 경로 추가\n\nwarnings.filterwarnings('ignore')\n\n# 한글 폰트\nplt.rc('font', family='AppleGothic')\nplt.rcParams['axes.unicode_minus'] = False\n\n#%%\nprint(\"=\"*60)\nprint(\"1. 환경 설정\")\nprint(\"=\"*60)\n\n# Random Seed\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nprint(f\"Device: {device}\")\n\n# Hyperparameters\nEMB_DIM = 64\nN_LAYERS = 3\nLR = 1e-3\nWEIGHT_DECAY = 1e-5\nEPOCHS = 100\nBATCH_SIZE = 2048\nNUM_NEG = 4\nHARD_NEG_RATIO = 0.5\n\n# SimGCL 전용 하이퍼파라미터\nEPS = 0.1  # Noise level for perturbation\nLAMBDA_CL = 0.2  # Contrastive loss weight\nTEMPERATURE = 0.2  # Temperature for InfoNCE\n\nprint(f\"\\n하이퍼파라미터:\")\nprint(f\"  임베딩 차원: {EMB_DIM}\")\nprint(f\"  레이어 수: {N_LAYERS}\")\nprint(f\"  학습률: {LR}\")\nprint(f\"  배치 크기: {BATCH_SIZE}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Noise (eps): {EPS}\")\nprint(f\"  Lambda CL: {LAMBDA_CL}\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. 데이터 로드\")\nprint(\"=\"*60)\n\n# 분할 데이터\ntrain_df = pd.read_csv('/kaggle/input/amazon/train_split.csv')\nval_df = pd.read_csv('/kaggle/input/amazon/val_split.csv')\ntest_df = pd.read_csv('/kaggle/input/amazon/test_split.csv')\n\nprint(f\"Train: {len(train_df):,}\")\nprint(f\"Val: {len(val_df):,}\")\nprint(f\"Test: {len(test_df):,}\")\n\n# 그래프 데이터\ngraph_data = torch.load('/kaggle/input/amazon/train_graph.pt')\nedge_index = graph_data['edge_index'].to(device)\ncca_edge_weight = graph_data['cca_weight'].to(device)\nn_users = graph_data['n_users']\nn_items = graph_data['n_items']\n\nprint(f\"\\n그래프 정보:\")\nprint(f\"  유저 수: {n_users:,}\")\nprint(f\"  아이템 수: {n_items:,}\")\nprint(f\"  엣지 수: {edge_index.shape[1]:,}\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. Negative Sampling 함수\")\nprint(\"=\"*60)\n\ndef fast_sample_negatives(batch_size, num_neg=4):\n    \"\"\"빠른 랜덤 negative sampling\"\"\"\n    return torch.randint(0, n_items, (batch_size, num_neg), device=device)\n\n@torch.no_grad()\ndef hard_negative_sampling(user_emb, item_emb, pos_users, num_neg=4, num_candidates=50):\n    \"\"\"Hard Negative Sampling\"\"\"\n    batch_size = len(pos_users)\n    candidates = torch.randint(0, n_items, (batch_size, num_candidates), device=device)\n    \n    user_expanded = user_emb[pos_users].unsqueeze(1)\n    item_candidates = item_emb[candidates]\n    scores = (user_expanded * item_candidates).sum(dim=2)\n    \n    _, top_indices = scores.topk(num_neg, dim=1)\n    hard_negs = candidates.gather(1, top_indices)\n    \n    return hard_negs\n\nprint(\"✅ Negative Sampling 함수 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. 평가 함수\")\nprint(\"=\"*60)\n\n@torch.no_grad()\ndef evaluate_recall_ndcg(model, eval_df, edge_index, edge_weight, k_list=[20, 50]):\n    \"\"\"Recall@K 및 NDCG@K 평가\"\"\"\n    model.eval()\n    u_emb, i_emb = model(edge_index, edge_weight, perturbed=False)\n    \n    user_groups = eval_df.groupby('user_idx')\n    \n    recall_at_k = {k: [] for k in k_list}\n    ndcg_at_k = {k: [] for k in k_list}\n    \n    for user_idx, group in user_groups:\n        gt_items = set(group['item_idx'].values)\n        \n        user_vec = u_emb[user_idx]\n        scores = (user_vec @ i_emb.t()).cpu().numpy()\n        \n        for k in k_list:\n            top_k_items = np.argsort(scores)[-k:][::-1]\n            \n            # Recall@K\n            hits = len(set(top_k_items) & gt_items)\n            recall = hits / len(gt_items) if len(gt_items) > 0 else 0\n            recall_at_k[k].append(recall)\n            \n            # NDCG@K\n            dcg = sum([1 / np.log2(i + 2) if item in gt_items else 0 \n                      for i, item in enumerate(top_k_items)])\n            idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(gt_items)))])\n            ndcg = dcg / idcg if idcg > 0 else 0\n            ndcg_at_k[k].append(ndcg)\n    \n    metrics = {}\n    for k in k_list:\n        metrics[f'Recall@{k}'] = np.mean(recall_at_k[k])\n        metrics[f'NDCG@{k}'] = np.mean(ndcg_at_k[k])\n    \n    return metrics\n\nprint(\"✅ 평가 함수 정의 완료\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"5. SimGCL 모델 학습\")\nprint(\"=\"*60)\n\n# 모델 초기화\nmodel = LightGCN_SimGCL(n_users, n_items, EMB_DIM, N_LAYERS, eps=EPS).to(device)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# 학습 데이터\ntrain_users = torch.LongTensor(train_df['user_idx'].values).to(device)\ntrain_items = torch.LongTensor(train_df['item_idx'].values).to(device)\ntrain_ratings = torch.FloatTensor(train_df['rating'].values).to(device)\n\n# [핵심] Rating을 가중치로 변환 (1점=0.5x, 3점=1.0x, 5점=2.0x)\n# Formula: weight = 0.3 + 0.34 * rating (1점: 0.64, 3점: 1.32, 5점: 2.0)\ntrain_weights = 0.3 + 0.34 * train_ratings\ntrain_weights = train_weights.to(device)\n\n# 학습 이력\nhistory = {\n    'loss': [], \n    'bpr_loss': [], \n    'cl_loss': [],\n    'val_recall@20': [], \n    'val_ndcg@20': []\n}\n\nprint(\"학습 시작...\")\nbest_val_recall = 0\npatience_counter = 0\nPATIENCE = 15\n\nfor epoch in range(EPOCHS):\n    model.train()\n    perm = torch.randperm(len(train_users), device=device)\n    epoch_loss = 0\n    epoch_bpr = 0\n    epoch_cl = 0\n    n_batches = 0\n    \n    for i in range(0, len(train_users), BATCH_SIZE):\n        batch_idx = perm[i:i+BATCH_SIZE]\n        pos_u = train_users[batch_idx]\n        pos_i = train_items[batch_idx]\n        weights = train_weights[batch_idx]\n        \n        # ========== Weighted BPR Loss ==========\n        u_emb, i_emb = model(edge_index, cca_edge_weight, perturbed=False)\n        \n        # Hard + Random Negative Sampling\n        n_hard = int(NUM_NEG * HARD_NEG_RATIO)\n        hard_negs = hard_negative_sampling(u_emb, i_emb, pos_u, num_neg=n_hard)\n        rand_negs = fast_sample_negatives(len(batch_idx), NUM_NEG - n_hard)\n        neg_i = torch.cat([hard_negs, rand_negs], dim=1)\n        \n        # BPR Loss with Rating Weights\n        pos_scores = (u_emb[pos_u] * i_emb[pos_i]).sum(dim=1)\n        neg_scores = (u_emb[pos_u].unsqueeze(1) * i_emb[neg_i]).sum(dim=2)\n        diff = pos_scores.unsqueeze(1) - neg_scores\n        loss_bpr_per_sample = -torch.log(torch.sigmoid(diff) + 1e-8).mean(dim=1)\n        loss_bpr = (loss_bpr_per_sample * weights).mean()\n        \n        # ========== Contrastive Loss (InfoNCE) ==========\n        # Perturbed views 생성\n        u_emb_1, i_emb_1, u_emb_2, i_emb_2 = model.get_perturbed_embeddings(edge_index, cca_edge_weight)\n        \n        # User Contrastive Loss (배치 내에서)\n        u_cl = compute_infonce_loss(u_emb_1[pos_u], u_emb_2[pos_u], temperature=TEMPERATURE)\n        \n        # Item Contrastive Loss (배치 내에서)\n        i_cl = compute_infonce_loss(i_emb_1[pos_i], i_emb_2[pos_i], temperature=TEMPERATURE)\n        \n        loss_cl = (u_cl + i_cl) / 2\n        \n        # ========== Total Loss ==========\n        total_loss = loss_bpr + LAMBDA_CL * loss_cl\n        \n        # Backward\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n        \n        epoch_loss += total_loss.item()\n        epoch_bpr += loss_bpr.item()\n        epoch_cl += loss_cl.item()\n        n_batches += 1\n    \n    scheduler.step()\n    avg_loss = epoch_loss / n_batches\n    avg_bpr = epoch_bpr / n_batches\n    avg_cl = epoch_cl / n_batches\n    \n    history['loss'].append(avg_loss)\n    history['bpr_loss'].append(avg_bpr)\n    history['cl_loss'].append(avg_cl)\n    \n    # Validation (매 5 epoch)\n    if (epoch + 1) % 5 == 0:\n        val_metrics = evaluate_recall_ndcg(model, val_df, edge_index, cca_edge_weight, k_list=[20])\n        history['val_recall@20'].append(val_metrics['Recall@20'])\n        history['val_ndcg@20'].append(val_metrics['NDCG@20'])\n        \n        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | \"\n              f\"BPR: {avg_bpr:.4f} | CL: {avg_cl:.4f} | \"\n              f\"Val Recall@20: {val_metrics['Recall@20']:.4f} | \"\n              f\"Val NDCG@20: {val_metrics['NDCG@20']:.4f}\")\n        \n        # Early Stopping\n        if val_metrics['Recall@20'] > best_val_recall:\n            best_val_recall = val_metrics['Recall@20']\n            patience_counter = 0\n            # 모델 저장\n            torch.save(model.state_dict(), 'models/simgcl_best.pt')\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(f\"\\n✅ SimGCL 모델 학습 완료 (Best Val Recall@20: {best_val_recall:.4f})\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"6. 학습 곡선 시각화\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Total Loss\naxes[0].plot(history['loss'], label='Total Loss', color='blue')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Total Training Loss')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# BPR vs CL Loss\naxes[1].plot(history['bpr_loss'], label='BPR Loss', color='orange')\naxes[1].plot(history['cl_loss'], label='CL Loss', color='green')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('BPR vs Contrastive Loss')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# Validation Metrics\nif history['val_recall@20']:\n    x = np.arange(5, len(history['loss']) + 1, 5)[:len(history['val_recall@20'])]\n    axes[2].plot(x, history['val_recall@20'], label='Recall@20', marker='o', color='purple')\n    axes[2].plot(x, history['val_ndcg@20'], label='NDCG@20', marker='s', color='red')\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Score')\n    axes[2].set_title('Validation Metrics')\n    axes[2].legend()\n    axes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('outputs/simgcl_training_curves.png', dpi=150)\nplt.show()\n\nprint(\"✅ 학습 곡선 저장 완료 (outputs/simgcl_training_curves.png)\")\n\n#%%\nprint(\"\\n\" + \"=\"*60)\nprint(\"7. 최종 모델 저장\")\nprint(\"=\"*60)\n\n# 최종 모델 로드 (Best validation)\nmodel.load_state_dict(torch.load('models/simgcl_best.pt'))\n\n# 전체 저장\ntorch.save({\n    'state_dict': model.state_dict(),\n    'config': {\n        'n_users': n_users,\n        'n_items': n_items,\n        'emb_dim': EMB_DIM,\n        'n_layers': N_LAYERS,\n        'eps': EPS\n    },\n    'history': history\n}, 'models/simgcl_final.pt')\n\nprint(\"✅ 모델 저장 완료:\")\nprint(\"  - models/simgcl_best.pt\")\nprint(\"  - models/simgcl_final.pt\")\n\nprint(\"\\n✅ SimGCL 학습 파이프라인 실행 완료!\")\n\n# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference (추론)\n",
    "학습된 모델로 테스트 데이터에 대한 추천을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference System\n",
    "class SimGCLInference:\n",
    "    def __init__(self, model_path, data_dir, graph_data, user2idx, item2idx, user_k, user_train_items):\n",
    "        self.device = device\n",
    "        \n",
    "        # Load model checkpoint\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        config = checkpoint['config']\n",
    "        \n",
    "        # Store mappings\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.user_k = user_k\n",
    "        self.user_train_items = user_train_items\n",
    "        \n",
    "        # Load graph\n",
    "        self.edge_index = graph_data['edge_index'].to(self.device)\n",
    "        self.edge_weight = graph_data['cca_weight'].to(self.device)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = LightGCN_SimGCL(\n",
    "            config['n_users'],\n",
    "            config['n_items'],\n",
    "            config['emb_dim'],\n",
    "            config['n_layers'],\n",
    "            eps=config['eps']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        print('✅ SimGCL Inference 시스템 초기화 완료')\n",
    "        \n",
    "        # Pre-compute embeddings\n",
    "        with torch.no_grad():\n",
    "            self.user_emb, self.item_emb = self.model(self.edge_index, self.edge_weight, perturbed=False)\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        results = []\n",
    "        \n",
    "        for user_id, group in test_df.groupby('user'):\n",
    "            # Unknown user\n",
    "            if user_id not in self.user2idx:\n",
    "                for _, row in group.iterrows():\n",
    "                    results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "                continue\n",
    "            \n",
    "            u_idx = self.user2idx[user_id]\n",
    "            K = self.user_k.get(u_idx, 2)\n",
    "            MIN_K = 2\n",
    "            \n",
    "            items_to_score = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                item_id = row['item']\n",
    "                \n",
    "                if item_id not in self.item2idx:\n",
    "                    results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "                    continue\n",
    "                \n",
    "                i_idx = self.item2idx[item_id]\n",
    "                \n",
    "                # Skip already purchased items\n",
    "                if i_idx in self.user_train_items.get(u_idx, set()):\n",
    "                    results.append({'user': row['user'], 'item': row['item'], 'recommend': 'X'})\n",
    "                    continue\n",
    "                \n",
    "                items_to_score.append((i_idx, row))\n",
    "            \n",
    "            if not items_to_score:\n",
    "                continue\n",
    "            \n",
    "            # Batch scoring\n",
    "            item_indices = torch.LongTensor([i for i, _ in items_to_score]).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                scores = (self.user_emb[u_idx] * self.item_emb[item_indices]).sum(dim=1).cpu().numpy()\n",
    "            \n",
    "            # Top-K selection (README 50% rule)\n",
    "            num_recommend = max(MIN_K, min(K, len(scores) // 2))\n",
    "            top_indices = set(np.argsort(scores)[-num_recommend:])\n",
    "            \n",
    "            for idx, (item_idx, row) in enumerate(items_to_score):\n",
    "                recommend = 'O' if idx in top_indices else 'X'\n",
    "                results.append({'user': row['user'], 'item': row['item'], 'recommend': recommend})\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "print('Inference class defined successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on Test Set\n",
    "print('\\n=== 추론 시작 ===')\n",
    "\n",
    "# Initialize inference system\n",
    "inference = SimGCLInference(\n",
    "    model_path='models/simgcl_best.pt',\n",
    "    data_dir='/kaggle/input/amazon/',\n",
    "    graph_data=graph_data,\n",
    "    user2idx=user2idx,\n",
    "    item2idx=item2idx,\n",
    "    user_k=user_k,\n",
    "    user_train_items=user_train_items\n",
    ")\n",
    "\n",
    "# Load test data (use validation split as example)\n",
    "test_df = val_df[['user', 'item']].copy()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = inference.predict(test_df)\n",
    "\n",
    "# Display sample results\n",
    "print('\\n샘플 추천 결과:')\n",
    "print('='*50)\n",
    "print(f\"{'user':<15}{'item':<15}{'recommend':<10}\")\n",
    "for _, row in predictions.head(20).iterrows():\n",
    "    print(f\"{str(row['user']):<15}{str(row['item']):<15}{row['recommend']:<10}\")\n",
    "print('='*50)\n",
    "\n",
    "total_cnt = len(predictions)\n",
    "rec_cnt = len(predictions[predictions['recommend'] == 'O'])\n",
    "print(f'\\nTotal recommends: {rec_cnt}/{total_cnt} ({rec_cnt/total_cnt*100:.1f}%)')\n",
    "print(f'Not recommend: {total_cnt - rec_cnt}/{total_cnt}')\n",
    "\n",
    "# Save results\n",
    "predictions.to_csv('outputs/predictions.csv', index=False)\n",
    "print('\\n✅ 추론 완료! 결과 저장: outputs/predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}